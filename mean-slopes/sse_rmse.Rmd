---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Sum of squares, root mean square

The [mean and slopes page](mean_and_slopes) used the Sum of Squared Error (SSE)
as the measure of how well a particular slope fits the data.

Here we will think about another, derived measure, called the Root Mean Square Error.

First, let us go back to the original problem.

## Hemoglobin and Packed Cell Volume, again

```{python}
# Run this cell.
import numpy as np
import matplotlib.pyplot as plt
# Make plots look a little bit more fancy
plt.style.use('fivethirtyeight')
# Print to 4 decimal places, show tiny values as 0
np.set_printoptions(precision=4, suppress=True)
import pandas as pd
pd.set_option('mode.copy_on_write', True)
```

We go back to the [data on chronic kidney
disease](../data/chronic_kidney_disease).

Download the data to your computer via this link: {download}`ckd_clean.csv
<../data/ckd_clean.csv>`.

We were interested to find the "best" slope for a line that relates the blood
hemoglobin measures in each patient, with their corresponding Packed Cell
Volume (PCV) blood measure:

```{python}
# Run this cell
ckd = pd.read_csv('ckd_clean.csv')
hgb = np.array(ckd['Hemoglobin'])
pcv = np.array(ckd['Packed Cell Volume'])
# Plot HGB on the x axis, PCV on the y axis
plt.plot(hgb, pcv, 'o')
plt.xlabel('Hemoglobin concentration')
plt.ylabel('Packed cell volume')
```

We are assuming our line goes through `hgb == 0` and `pcv == 0`, on the basis
that the red blood cells measured by PCV contain all the hemoglobin, so if PCV
is 0 then hemoglobin must be 0 and vice versa.  Our *predicted* values for PCV
for each patient are the given by some slope`s` multiplied by the corresponding
hemoglobin value for that patient.

For example, say the slope is 2.25, then the predicted PCV values are:

```{python}
s = 2.25
predicted_pcv = s * hgb
predicted_pcv[:10]
```

The *errors* for these predictions are:

```{python}
errors = pcv - predicted_pcv
errors[:10]
```

## Sum of Squared error

We decided to use the Sum of Squared Error (SSE) as a measure of the quality of
fit.

The sum of squared error is nothing but:

```{python}
sse = np.sum(errors ** 2)
sse
```

Here's a function to return the SSE.  We call this the *cost function* because
it calculates a *cost* in terms of error for a given slope.

```{python}
def calc_sse(slope):
    predicted_pcv = hgb * slope  # 'hgb' comes from the top level
    errors = pcv - predicted_pcv # 'pcv' comes from the top level
    return np.sum(errors ** 2)
```

```{python}
# Recalculate SSE for slope of interest.
calc_sse(2.25)
```

Then we used `minimize` to find the slope that minimizes the SSE:

```{python}
from scipy.optimize import minimize
# 2.25 below is the slope value to start the search.
res_sse = minimize(calc_sse, 2.25)
res_sse
```

Notice the warning - there was some inaccuracy in the calculation.   We  may try slightly different cost functions to avoid that.

## Root mean squared error

One problem with the SSE is that it can get very large, especially if there are
a large number of points.  When the values get large, this can cause
calculation problems.  Very large numbers for SSE can make
it harder to think about the SSE values.

One way of allowing for the number of points, is to use the *mean* square error
(MSE), instead of the SSE.  Here is the MSE cost function.

```{python}
def calc_mse(slope):
    predicted_pcv = hgb * slope  # 'hgb' comes from the top level
    errors = pcv - predicted_pcv # 'pcv' comes from the top level
    return np.mean(errors ** 2)  # Notice mean instead of sum
```

We try `minimize` again, to find the best slope.

```{python}
# 2.25 below is the slope value to start the search.
res_mse = minimize(calc_mse, 2.25)
res_mse
```

Notice there is no warning this time in the optimization message.


Notice that running `minimize` on the MSE found the same slope as it did for
the SSE. The resulting cost function `fun` value is the original SSE value
divided by the number of points. (It's not exactly the same because of the
inaccuracy of the SSE estimate).

```{python}
res_sse.fun / len(hgb)
```

It's also common to use the *square root* of the MSE, or the Root Mean Square
Error.  This has one advantage that it's easier to compare the RMSE to the
individual errors — we don't normally think easily in terms of squared error.
This is the RMSE cost function — another tiny variation.

```{python}
def calc_rmse(slope):
    predicted_pcv = hgb * slope  # 'hgb' comes from the top level
    errors = pcv - predicted_pcv # 'pcv' comes from the top level
    return np.sqrt(np.mean(errors ** 2))  # Notice square root of mean.
```

```{python}
# 2.25 below is the slope value to start the search.
res_rmse = minimize(calc_rmse, 2.25)
res_rmse
```

The cost function `fun` value is just the square root of the MSE `fun` value:

```{python}
np.sqrt(res_mse.fun)
```

Notice again that `minimize` on RMSE found exactly the same slope as for MSE and SSE.   Why?

Reflect a little, and then read on.

(monotonicity)=

## Monotonicity

SSE, MSE, and RMSE give the same answer from `minimize` because they are
*monotonic* with respect to each other.

This means that the value that comes back from the SSE, MSE or RMSE cost
function varies
[monotonically](https://en.wikipedia.org/wiki/Monotonic_function) with respect
to the other.

For the moment, lets consider SSE and RMSE.  Consider any two slope values. Call the
first slope `s1` and the second slope `s2`.  Say `s1` gives an SSE value of
`L1` and `s2` gives a likelihood value of `L2`. Now say we've got another way
of calculating a value for `s1` and `s2` (such as RMSE), and the corresponding
values that come back from the new calculation are `N1` and `N2`.  Remember
`miminize` is searching for the slope value that gives the lowest value from
the cost function.  So, we can swap the new calculation into our cost function,
and get the same parameters back from `miminize`, as long as it is always true
that:

* when `L1 > L2` then `N1 > N2`;
* when `L1 < L2` then `N1 < N2`; and
* when `L1 == L2`, `N1 == N2`.

In that case we call our new calculation generating `N1` and `N2` *monotonic*
with respect to the original calculation generating `L1` and `L2`.  In our case
this original calculation is SSE. The new monotonic calculation is the *RMSE*.

In our case, this means we can minimize on the root mean squared error, and we
are guaranteed to get the same result within calculation precision, as we would
for the sum of squared error.
